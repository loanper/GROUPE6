{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"},{"sourceId":8960184,"sourceType":"datasetVersion","datasetId":5393045},{"sourceId":77689,"sourceType":"modelInstanceVersion","modelInstanceId":65323,"modelId":89875}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-07-15T15:12:40.156209Z","iopub.execute_input":"2024-07-15T15:12:40.156846Z","iopub.status.idle":"2024-07-15T15:12:40.339839Z","shell.execute_reply.started":"2024-07-15T15:12:40.156813Z","shell.execute_reply":"2024-07-15T15:12:40.338965Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"First lets import the data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-15T15:12:41.10854Z","iopub.execute_input":"2024-07-15T15:12:41.109454Z","iopub.status.idle":"2024-07-15T15:12:41.136448Z","shell.execute_reply.started":"2024-07-15T15:12:41.109421Z","shell.execute_reply":"2024-07-15T15:12:41.135384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"print(\"Null values per column:\")\nprint(train.isnull().sum())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## finding realtionships","metadata":{}},{"cell_type":"markdown","source":"### Survival rate by Sex:","metadata":{}},{"cell_type":"markdown","source":"##### for sex we clearly see that the women had a great advantage for Surviving, \n##### mostly because of the (women and children first) protocols that  was used before","metadata":{}},{"cell_type":"code","source":"\n# Calculate survival rates by Sex\nsurvival_by_sex = train.groupby('Sex')['Survived'].mean()\nprint(\"Survival rate by Sex:\")\nprint(survival_by_sex)\nprint(\"\\n\")\n\n# Plot survival rate by Sex\nplt.figure(figsize=(8, 6))\nsns.barplot(x='Sex', y='Survived', data=train, ci=None, palette='viridis')\nplt.title('Survival Rate by Sex')\nplt.ylabel('Survival Rate')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### This stark difference highlights gender as a critical factor influencing survival.","metadata":{}},{"cell_type":"markdown","source":"### Survival rate by Age Group:","metadata":{}},{"cell_type":"markdown","source":"##### Children (babies and young children) had notably higher survival rates, likely due to prioritization in lifeboat allocation.\n##### Adults and teenagers generally experienced lower survival rates, potentially reflecting the prioritization of vulnerable groups and physical limitations during evacuation.\n##### The significantly lower survival rate for older passengers (> 60 years) suggests challenges faced by elderly individuals in emergency evacuation scenarios.","metadata":{}},{"cell_type":"code","source":"train['AgeGroup'] = pd.cut(train['Age'], bins=[0, 5, 14, 18, 30, 60, 100], labels=['Baby', 'Child', 'Teenager', 'Adult', 'OldAdult', 'Old'])\nsurvival_by_age_group = train.groupby('AgeGroup')['Survived'].mean()\nprint(\"Survival rate by Age Group:\")\nprint(survival_by_age_group)\nprint(\"\\n\")\n\n# Plot survival rate by Age Group\nplt.figure(figsize=(8, 6))\nsns.barplot(x='AgeGroup', y='Survived', data=train, ci=None, order=['Baby', 'Child', 'Teenager', 'Adult', 'OldAdult', 'Old'], palette='viridis')\nplt.title('Survival Rate by Age Group')\nplt.ylabel('Survival Rate')\nplt.xlabel('Age Group')\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Survival rate by Pclass:","metadata":{}},{"cell_type":"markdown","source":"##### First-class passengers (62.96% survival rate) had higher survival rates, possibly due to their proximity to lifeboats and priority access during evacuation.\n##### Third-class passengers (24.24% survival rate) faced the lowest survival rates, likely due to their lower deck locations and delayed access to lifeboats.\n##### Passenger class serves as a proxy for socio-economic status, highlighting disparities in survival linked to wealth and access to resources during the Titanic disaster.","metadata":{}},{"cell_type":"code","source":"survival_by_pclass = train.groupby('Pclass')['Survived'].mean()\nprint(\"Survival rate by Pclass:\")\nprint(survival_by_pclass)\nprint(\"\\n\")\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x='Pclass', y='Survived', data=train, palette='viridis')\nplt.title('Survival Rate by Passenger Class')\nplt.xlabel('Passenger Class')\nplt.ylabel('Survival Rate')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Survival rate by Fare Group:","metadata":{}},{"cell_type":"markdown","source":"##### Passengers who paid higher fares (e.g., Very High and Luxury fare classes) had significantly higher survival rates, likely reflecting their accommodations closer to lifeboats and priority boarding.\n##### Conversely, passengers who paid lower fares (e.g., Very Low fare class) had a 0% survival rate, underscoring challenges faced by passengers in lower fare categories during the evacuation.\n##### Fare class serves as a proxy for passenger location on the ship and access to evacuation resources, illustrating its impact on survival outcomes.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/titanic/train.csv')\n\nprint((train['Fare'].values < 5).sum())\n\ntrain['FareClass'] = pd.cut(train['Fare'], bins= [0, 5, 15, 25, 50, 100, 250, float('inf')],\n                         labels=['Very Low', 'Low', 'Moderate', 'Medium', 'High', 'Very High', 'Luxury'])\nsurvival_by_fare = train.groupby('FareClass')['Survived'].mean()\nprint(\"Survival rate by Fare Group:\")\nprint(survival_by_fare)\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x='FareClass', y='Survived', data=train, ci=None, order=['Very Low', 'Low', 'Moderate', 'Medium', 'High', 'Very High', 'Luxury'], palette='viridis')\nplt.title('Survival Rate by Fare Group')\nplt.ylabel('Survival Rate')\nplt.xlabel('Fare Group')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing and Feature Engineering:","metadata":{}},{"cell_type":"markdown","source":"1. Handling Missing Values:\n Age and Fare: Missing age values were imputed using the K-Nearest Neighbors (KNN) approach, leveraging \ninformation from similar observations to fill in gaps in the dataset. Fare values missing due to rare occurrences \nwere replaced with the median fare.\n- Embarked: Missing embarkation values were filled with the most frequent embarkation port recorded in the \ndataset.\n- Cabin: Cabin information, initially sparse, was simplified by mapping cabins to their first letter and filling \nmissing values with 'N', indicating no cabin data.\n2. Feature Engineering:\n- Family Size: Created a 'Family' feature by summing 'Parch' (number of parents/children aboard) and 'SibSp' \n(number of siblings/spouses aboard), providing insight into passenger family relationships.\n- Title Extraction: Extracted passenger titles (e.g., Mr., Mrs., Miss) from names, categorizing rare titles into a \nconsolidated 'Rare' category and merging similar titles (e.g., 'Mlle' and 'Ms' into 'Miss').\n- Age and Fare Categorization: Binned age and fare values into discrete categories ('Baby', 'Child', 'Teenager', \n'Adult', 'OldAdult', 'Old' for age; 'Very Low' to 'Luxury' for fare), allowing for categorical analysis of survival \ntrends based on age and economic status.\n- Alone Status: Determined whether passengers traveled alone ('Alone') based on the absence of family \nmembers onboard.\n3. One-Hot Encoding:\n- Transformed categorical variables such as 'Sex', 'Embarked', 'Title', and 'Cabin' into binary indicators using one\u0002hot encoding, facilitating the incorporation of categorical data into machine learning models without imposing \nordinality.\n4. Feature Selection and Scaling:\n- Selected relevant features including demographic attributes ('Age', 'Sex', 'Pclass'), family information ('Family', \n'Alone'), socio-economic indicators ('Fare', 'Title'), and cabin categories.\n- Scaled numeric features ('Age', 'Fare', 'Parch', 'SibSp') using standard scaling to normalize data distributions and \nensure all features contribute equally to model training.\n","metadata":{}},{"cell_type":"code","source":"#the function used in the preprocessing\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\n\ndef pre(df):\n\n    df['Family'] = df['Parch'] + df['SibSp']\n    \n    # Extract titles from names\n    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df['Title'] = df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', \n                                       'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n\n    # Fill missing embarked with the most frequent value\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n\n    # Fill missing fare with the median\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n\n    # Create fare classes\n    df['Very Low'] = (df['Fare'] <= 4)\n    df['Low'] = (df['Fare'] > 4) & (df['Fare'] <= 15)\n    df['Moderate'] = (df['Fare'] > 15) & (df['Fare'] <= 25)\n    df['Medium'] = (df['Fare'] > 25) & (df['Fare'] <= 50)\n    df['High'] = (df['Fare'] > 50) & (df['Fare'] <= 100)\n    df['Very High'] = (df['Fare'] > 100) & (df['Fare'] <= 250)\n    df['Luxury'] = (df['Fare'] > 250)\n\n    # Convert cabin to first letter and fill missing values with 'N'\n    df['Cabin'] = df['Cabin'].fillna('N').map(lambda x: x[0])\n    \n    # Fill missing age with the median or using KNN Imputer\n    age_imputer = KNNImputer(n_neighbors=5)\n    df['Age'] = age_imputer.fit_transform(df[['Age']])\n    \n    # Create age categories\n    df['Baby'] = (df['Age'] <= 5)\n    df['Child'] = (df['Age'] > 5) & (df['Age'] <= 14)\n    df['Teenager'] = (df['Age'] > 14) & (df['Age'] <= 18)\n    df['Adult'] = (df['Age'] > 18) & (df['Age'] <= 30)\n    df['OldAdult'] = (df['Age'] > 30) & (df['Age'] <= 60)\n    df['Old'] = (df['Age'] > 60)\n    df['Alone'] = df['Family'] == 0\n    \n    # One-hot encoding\n    df = pd.get_dummies(df, columns=['Sex', 'Embarked', 'Title', 'Cabin'], drop_first=True)\n    #in case there is a cabin that was not mentioned, create it and fill it with False\n    for l in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T']:\n        col = f'Cabin_{l}'\n        if col not in df.columns:\n            df[col] = pd.Series([False]*df.shape[0])\n            df[col].fillna(False, inplace=True)\n    \n    # Select relevant columns\n    cols = ['Age', 'Fare', 'Family', 'Parch', 'SibSp', 'Alone', 'Pclass', 'Very Low', 'Low', 'Moderate', 'Medium',\n       'High', 'Very High', 'Luxury', 'Baby', 'Child', 'Teenager',\n       'Adult', 'OldAdult', 'Old', 'Sex_male',\n       'Embarked_Q', 'Embarked_S', 'Title_Miss', 'Title_Mr', 'Title_Mrs',\n       'Title_Rare', 'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F',\n       'Cabin_G', 'Cabin_T'] #Cabin_N was ignored\n    df2 = df[cols].copy()\n    \n    # Scale numeric features\n    numeric_features = ['Age', 'Fare', 'Parch', 'SibSp']\n    scaler = StandardScaler()\n    df2[numeric_features] = scaler.fit_transform(df2[numeric_features])\n    \n    return df2\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T15:05:53.872571Z","iopub.execute_input":"2024-07-15T15:05:53.873301Z","iopub.status.idle":"2024-07-15T15:05:54.094465Z","shell.execute_reply.started":"2024-07-15T15:05:53.873261Z","shell.execute_reply":"2024-07-15T15:05:54.093457Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modling","metadata":{}},{"cell_type":"markdown","source":"##### first lets try diffrent models\n- Logistic Regression\n- Support Vector Machine (SVM)\n- K-Nearest Neighbors (KNN)\n- Random Forest\n- Gradient Boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\n#load the data again\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n# Perform data preprocessing and feature engineering (assuming you've already done this)\ntrain_processed = pre(train)\n\n# Define features and target variable\nX = train_processed\ny = train['Survived']\n\n# Split the data into train and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Initialize classifiers\nclassifiers = {\n    'Logistic Regression': LogisticRegression(),\n    'SVM': SVC(),\n    'KNN': KNeighborsClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'Gradient Boosting': GradientBoostingClassifier()\n}\n\n# Dictionary to store results\nresults = {'Classifier': [], 'Test Accuracy': []}\n\n# Iterate over classifiers\nfor clf_name, clf in classifiers.items():\n    # Fit the model\n    clf.fit(X_train, y_train)\n    \n    # Predict on test set\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    # Store results\n    results['Classifier'].append(clf_name)\n    results['Test Accuracy'].append(accuracy)\n\n# Create DataFrame from results\nresults_df = pd.DataFrame(results)\n\n# Display the results\nresults_df\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Analysis:\n- Random Forest emerged as the top performer among the classifiers evaluated, achieving a test accuracy of \n79.6%. This ensemble method likely benefited from its ability to capture complex interactions in the data and \nhandle non-linear relationships effectively.\n- SVM and Gradient Boosting also demonstrated strong performance with accuracies of 78.4% and 78.3%, \nrespectively. SVM's capability to find optimal boundaries in high-dimensional spaces and Gradient Boosting's \niterative improvement on misclassified instances contributed to their competitive accuracy.\n- Logistic Regression, a simpler linear model, achieved a respectable accuracy of 77.6%. While less complex, its \ninterpretability and efficiency in handling linear relationships make it a viable baseline model.\n- KNN performed slightly lower with a test accuracy of 74.5%, indicating that its performance might be sensitive \nto the choice of distance metric and number of neighbors selected.","metadata":{}},{"cell_type":"markdown","source":"##### To expedite the tuning process, a Decision Tree was initially selected for its efficiency compared to ensemble methods like Random Forest","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random'],\n    'max_depth': range(8, 13),\n    'min_samples_split': range(6, 11),\n    'min_samples_leaf': range(1, 4),\n    'max_features': ['sqrt', 'log2', None],\n    'random_state': [0],\n    'max_leaf_nodes': range(80, 101, 2),\n    'class_weight': [{0: 1, 1: w} for w in np.linspace(1.3, 1.7, 5)],\n    'ccp_alpha': np.linspace(4e-05, 6e-05, 5)\n}\n\n# Create a Decision Tree classifier\ndt_classifier = DecisionTreeClassifier()\n\n# Instantiate GridSearchCV\ngrid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n\n# Fit GridSearchCV\ngrid_search.fit(X_train, y_train)\n\n# Print the best parameters and best score\nprint(\"Best Parameters found:\")\nprint(grid_search.best_params_)\nprint(\"Best Accuracy Score:\")\nprint(grid_search.best_score_)\n\ny_pred = grid_search.best_estimator_.predict(X_test)\n    \n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\naccuracy\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Best Parameters Found:\n- Criterion: 'Gini'\n- Splitter: 'Best'\n- Max Depth: 12\n- Min Samples Split: 8\n- Min Samples Leaf: 1\n- Max Features: 'sqrt'\n- Random State: 0\n- Max Leaf Nodes: 84\n- Class Weight: {0: 1, 1: 1.5}\n- CCP Alpha: 4e-05\n- Accuracy: 81.7%\n","metadata":{}},{"cell_type":"code","source":"base_params = {\n    'criterion': 'gini',\n    'splitter': 'best',\n    'max_depth': 10,\n    'min_samples_split': 8,\n    'min_samples_leaf': 1,\n    'max_features': 'sqrt',\n    'random_state': 0,\n    'max_leaf_nodes': 84,\n    'class_weight': {0: 1, 1: 1.5},\n    'ccp_alpha': 4e-05\n}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Using the optimized Decision Tree parameters as a base, three ensemble methods were explored through additional GridSearchCV:","metadata":{}},{"cell_type":"markdown","source":"- BaggingClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid for BaggingClassifier\nparam_grid_bagging = {\n    'n_estimators': range(50, 351, 50),\n    'max_samples': [0.5, 0.7, 1.0],\n    'max_features': [0.5, 0.7, 1.0]\n}\n\n# Instantiate the base model\nbase_model = DecisionTreeClassifier(**base_params)\n\n# Create BaggingClassifier\nbagging_model = BaggingClassifier(estimator=base_model, random_state=0)\n\n# Instantiate GridSearchCV\ngrid_search_bagging = GridSearchCV(estimator=bagging_model, param_grid=param_grid_bagging, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n\n# Fit GridSearchCV\ngrid_search_bagging.fit(X_train, y_train)\n\n# Print the best parameters and best score\nprint(\"BaggingClassifier Best Parameters found:\")\nprint(grid_search_bagging.best_params_)\nprint(\"BaggingClassifier Best Accuracy Score:\")\nprint(grid_search_bagging.best_score_)\n\ny_pred = grid_search_bagging.best_estimator_.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\naccuracy\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- RandomForestClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Define the parameter grid for RandomForestClassifier\nparam_grid_rf = {\n    'n_estimators': range(60, 361, 50),\n    'max_features': ['sqrt', 'log2', None]\n}\n\n# Create RandomForestClassifier with the base parameters\nrf_model = RandomForestClassifier(\n    criterion=base_params['criterion'],\n    max_depth=base_params['max_depth'],\n    min_samples_split=base_params['min_samples_split'],\n    min_samples_leaf=base_params['min_samples_leaf'],\n    max_leaf_nodes=base_params['max_leaf_nodes'],\n    class_weight=base_params['class_weight'],\n    ccp_alpha=base_params['ccp_alpha'],\n    random_state=1212\n)\n\n# Instantiate GridSearchCV\ngrid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n\n# Fit GridSearchCV\ngrid_search_rf.fit(X_train, y_train)\n\n# Print the best parameters and best score\nprint(\"RandomForestClassifier Best Parameters found:\")\nprint(grid_search_rf.best_params_)\nprint(\"RandomForestClassifier Best Accuracy Score:\")\nprint(grid_search_rf.best_score_)\n\ny_pred = grid_search_rf.best_estimator_.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\naccuracy\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- ExtraTreesClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\n# Define the parameter grid for ExtraTreesClassifier\nparam_grid_et = {\n    'n_estimators': range(50, 351, 50),\n    'max_features': ['sqrt', 'log2', None]\n}\n\n# Create ExtraTreesClassifier with the base parameters\net_model = ExtraTreesClassifier(\n    criterion=base_params['criterion'],\n    max_depth=base_params['max_depth'],\n    min_samples_split=base_params['min_samples_split'],\n    min_samples_leaf=base_params['min_samples_leaf'],\n    max_leaf_nodes=base_params['max_leaf_nodes'],\n    class_weight=base_params['class_weight'],\n    ccp_alpha=base_params['ccp_alpha'],\n    random_state=1212\n)\n\n# Instantiate GridSearchCV\ngrid_search_et = GridSearchCV(estimator=et_model, param_grid=param_grid_et, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n\n# Fit GridSearchCV\ngrid_search_et.fit(X_train, y_train)\n\n# Print the best parameters and best score\nprint(\"ExtraTreesClassifier Best Parameters found:\")\nprint(grid_search_et.best_params_)\nprint(\"ExtraTreesClassifier Best Accuracy Score:\")\nprint(grid_search_et.best_score_)\n\ny_pred = grid_search_et.best_estimator_.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\naccuracy\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Final model : ","metadata":{}},{"cell_type":"code","source":"rf_model = RandomForestClassifier(\n    n_estimators=110,\n    max_depth=12,\n    min_samples_split=8,\n    min_samples_leaf=1,\n    max_features=0.15,\n    random_state=900,\n    bootstrap=True\n) \nrf_model.fit(X_train, y_train)\n\ny_pred = rf_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\naccuracy\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Now lets prepare the whole train and test data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/titanic/train.csv')\n\ntrain_processed = pre(train)\n\nX = train_processed\ny = train['Survived']\n\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\n\ntest_processed = pre(test)\n\nids = test['PassengerId']\nX_t = test_processed","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Now lets fit the model on the whole data and predict the test data on kaggle","metadata":{}},{"cell_type":"code","source":"rf_model.fit(X, y)\n\npreds = rf_model.predict(X_t)\n\nids = test['PassengerId']\n\nPredictionDF = pd.DataFrame({'PassengerId' : ids, 'Survived' : preds})\n\nPredictionDF.to_csv('final_submissions.csv', index=False)\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### you can find the file 'final_submissions.csv' in the input of this notebook","metadata":{}},{"cell_type":"markdown","source":"##### finally, lets create a pipeline for all the work above and ssave it as picke file to import it later","metadata":{}},{"cell_type":"code","source":"import pickle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\npre_transformer = FunctionTransformer(pre, validate=False)\n\npipeline = Pipeline([\n    ('preprocessor', pre_transformer),\n    ('classifier', rf_model)\n])\n\npipeline.fit(train, y)\n\n# Save the pipeline to a pickle file\nwith open('Titanic_pipeline_model.pkl', 'wb') as file:\n    pickle.dump(pipeline, file)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### you can find the file 'Titanic_pipeline_model.pkl' in the input of this notebook","metadata":{}},{"cell_type":"markdown","source":"#### Now if you want, you could just download the files {Titanic_pipeline_model.pkl, test.csv} and run this only cell to get the prediction directly","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport pickle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nwith open('/kaggle/input/titanic-pipeline/scikitlearn/pickle_file/1/Titanic_pipeline_model.pkl', 'rb') as file:\n    loaded_pipeline = pickle.load(file)\n\n    test = pd.read_csv('/kaggle/input/titanic/test.csv')\n\npreds = loaded_pipeline.predict(test)\n\nids = test['PassengerId']\n\nPredictionDF = pd.DataFrame({'PassengerId' : ids, 'Survived' : preds})\n\nPredictionDF.to_csv('final_submissions.csv', index=False)\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}